{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stephen Iota\n",
    "\n",
    "\n",
    "iota@usc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training, dev and unlabeled test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following provides a starting code (Python 3) of how to read the labeled training and dev cipher text, and unlabeled test cipher text, into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, dev, test = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in open('./train_enc.tsv', encoding='utf-8'):\n",
    "#    x = x.rstrip('\\n\\r').split('\\t')\n",
    "#    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
    "#    x[0] = int(x[0]) \n",
    "#    train.append(x)\n",
    "#print (len(train))\n",
    "#print (train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in open('./dev_enc.tsv', encoding='utf-8'):\n",
    "#    x = x.rstrip('\\n\\r').split('\\t')\n",
    "#    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
    "#    x[0] = int(x[0]) \n",
    "#    dev.append(x)\n",
    "#print (len(dev))\n",
    "#print (dev[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different from 'train' and 'dev' that are both list of tuples, 'test' will be just a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in open('./test_enc_unlabeled.tsv', encoding='utf-8'):\n",
    "#    x = x.rstrip('\\n\\r')\n",
    "#    test.append(x)\n",
    "#print (len(test))\n",
    "#print (test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can split every sentence into lists of words by white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_split = [[x[0], x[1].split(' ')] for x in train]\n",
    "#dev_split = [[x[0], x[1].split(' ')] for x in dev]\n",
    "#test_split = [[x.split(' ')] for x in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CipherTxtData(Dataset):\n",
    "    \"\"\"Dataclass for reading cipher-text data from provided files.\"\"\"\n",
    "    PATH = {\"train\": \"data/train_enc.tsv\",\n",
    "            \"dev\": \"data/dev_enc.tsv\",\n",
    "            \"test\": \"data/test_enc_unlabeled.tsv\"}\n",
    "    def __init__(self,\n",
    "                 mode: str = \"train\",\n",
    "                 split: bool = True):\n",
    "        super().__init__()\n",
    "        self.data: list[list[str]] = []\n",
    "        self.split: bool = split\n",
    "        try:\n",
    "            self.root: str = self.PATH[mode]\n",
    "            self.mode: str = mode\n",
    "            self.read()\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Mode {mode} not supported.\")\n",
    "\n",
    "    def read(self) -> None:\n",
    "        \"\"\"Read datafile.\"\"\"\n",
    "        with open(self.PATH[self.mode], mode='r', encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self._read(line, mode=self.mode)\n",
    "        if self.split:\n",
    "            self._split()\n",
    "\n",
    "    def _read(self, line, mode):\n",
    "        if mode == \"test\":\n",
    "           x = line.rstrip('\\n\\r')\n",
    "        else:\n",
    "            x = line.rstrip('\\n\\r').split('\\t')\n",
    "            x[0] = int(x[0])\n",
    "        self.data.append(x)\n",
    "\n",
    "    def _split(self):\n",
    "        if self.mode == \"test\":\n",
    "            self.data = [[x.split(' ')] for x in self.data]\n",
    "        else:\n",
    "            self.data = [[x[0], x[1].split(' ')] for x in self.data]\n",
    "\n",
    "    @property\n",
    "    def X(self) -> Union[list, None]:\n",
    "        \"\"\"Documents.\"\"\"\n",
    "        if self.data is None:\n",
    "            return None\n",
    "        elif self.mode == \"test\":\n",
    "            return self.data\n",
    "        else:\n",
    "            return [x[1] for x in self.data] if self.data is not None else None\n",
    "\n",
    "\n",
    "    @property\n",
    "    def y(self) -> Union[list, None]:\n",
    "        \"\"\"Class labels, either 0 or 1\"\"\"\n",
    "        if self.mode != \"test\" and self.data is not None:\n",
    "            return [x[0] for x in self.data]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.data) if self.data is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Optional, Sequence, Callable\n",
    "\n",
    "# import gensim\n",
    "\n",
    "# class CipherCorpus:\n",
    "#     \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "#     def __init__(self, text):\n",
    "#         self.text = text\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         for line in self.text:\n",
    "#             yield line\n",
    "\n",
    "# class CipherNGramData(Dataset):\n",
    "#     \"\"\"Dataclass to yield ciphertext ngrams.\n",
    "\n",
    "#     If ngrams are retrieved using `__get_item__` method,\n",
    "#     returns one-hot encoding of ngrams.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, ciphertxtdata: CipherTxtData, context_size: int = 3):\n",
    "#         self.context_size = context_size\n",
    "#         self.text = [x for y in ciphertxtdata for x in y]\n",
    "#         self.vocab = set(self.text)\n",
    "#         self.vocab_size = len(self.vocab)\n",
    "#         self.word_to_idx = {word: i for i, word in enumerate(self.vocab)}\n",
    "\n",
    "#         self.ngrams = [\n",
    "#             [[self.text[i - j - 1] for j in range(self.context_size)], self.text[i]]\n",
    "#             for i in range(self.context_size, len(self.text))\n",
    "#         ]\n",
    "\n",
    "#         self.X = []\n",
    "#         self.y = []\n",
    "#         for i in range(self.context_size, len(self.text)):\n",
    "#             self.X.append([self.text[i - j - 1] for j in range(self.context_size)])\n",
    "#             self.y.append(self.text[i])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         X = self.X[i]\n",
    "#         X = torch.tensor([self.word_to_idx[w] for w in X], dtype=torch.long)\n",
    "#         y = self.y[i]\n",
    "#         y = torch.tensor([self.word_to_idx[y]])\n",
    "\n",
    "#         return X, y\n",
    "\n",
    "\n",
    "# class CipherW2VData(Dataset):\n",
    "#     \"\"\"Dataclass that generates w2v embeddings.\"\"\"\n",
    "#     OOV = '~'\n",
    "#     def __init__(self,\n",
    "#                  corpus: Union[CipherCorpus, list],\n",
    "#                  wv: Optional[gensim.models.Word2Vec] = None,\n",
    "#                  **w2vparams) -> None:\n",
    "#         super().__init__()\n",
    "#         if not isinstance(corpus, CipherCorpus):\n",
    "#             corpus = CipherCorpus(corpus)\n",
    "#         self.corpus = corpus\n",
    "\n",
    "#         if not wv:\n",
    "#             model = gensim.models.Word2Vec(sentences=self.corpus, **w2vparams)\n",
    "#             self.wv = model.wv\n",
    "#             del model\n",
    "#         else:\n",
    "#             self.wv = wv\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.corpus.text)\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         sentence = self.corpus.text[i]\n",
    "\n",
    "#         sentence_emb = []\n",
    "#         for word in sentence:\n",
    "#             if word in self.wv:\n",
    "#                 emb = self.wv[word]\n",
    "#             else:\n",
    "#                 emb = self.wv[self.OOV]\n",
    "#             emb = torch.tensor(emb)\n",
    "#             sentence_emb.append(emb)\n",
    "\n",
    "\n",
    "#         return sentence_emb\n",
    "\n",
    "\n",
    "# class CipherVecData(Dataset):\n",
    "#     \"\"\"General dataclass for ciphertext embeddings.\"\"\"\n",
    "#     def __init__(self, X: Sequence, y: Optional[Sequence] = None,\n",
    "#                  transform: Callable = None,\n",
    "#                  target_transform: Callable= None) -> None:\n",
    "#         super().__init__()\n",
    "#         self.X = X\n",
    "#         self.y = y\n",
    "#         self.transform = transform\n",
    "#         self.target_transfrom = target_transform\n",
    "\n",
    "#     def __len__(self,):\n",
    "#         return len(self.X) if self.X is not None else 0\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         x = self.X[i]\n",
    "#         if self.transform:\n",
    "#             x = self.transform(x)\n",
    "#         if self.target_transfrom and self.y is not None:\n",
    "#             y = self.target_transfrom(self.y[i])\n",
    "#         elif self.y is not None:\n",
    "#             y = self.y[i]\n",
    "\n",
    "#         return x, y if self.y is not None else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = CipherTxtData(mode=\"train\", split=False)\n",
    "devdata = CipherTxtData(mode=\"dev\", split=False)\n",
    "testdata = CipherTxtData(mode=\"test\", split=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may choose to experiment with different methods using your program. However, you need to embed the training and inference processes at here. We will use your prediction on the unlabeled test data to grade, while checking this part to understand how your method has produced the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(text, labels=None, file_name=\"test.txt\"):\n",
    "    with open(file_name, \"w\") as f:\n",
    "        if labels:\n",
    "            for seq, label in zip(text, labels):\n",
    "                f.write(f\"__label__{str(label)} {seq}\\n\")\n",
    "        else:\n",
    "            for seq in text:\n",
    "                f.write(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    predictions = []\n",
    "    for text in data:\n",
    "        pred = model.predict(text)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_scalers(predictions, truth):\n",
    "    score = 0\n",
    "    for pred, true in zip(predictions, truth):\n",
    "        if pred[0][0] == \"__label__1\" and true == 1:\n",
    "            score += 1\n",
    "        elif pred[0][0] == \"__label__0\" and true == 0:\n",
    "            score += 1\n",
    "    return score / len(truth)\n",
    "\n",
    "import torch\n",
    "@torch.no_grad()\n",
    "def accuracy_score_logits(logits: torch.tensor, y_true: torch.tensor,\n",
    "                          normalize: bool = True) -> Union[float, int]:\n",
    "    \"\"\"Score predictions against ref given logits,\n",
    "    i.e., argmax(logits[i]) = pred[i]\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    for pair, true in zip(logits, y_true):\n",
    "        pred = torch.argmax(pair)\n",
    "        if pred == true:\n",
    "            score += 1\n",
    "\n",
    "    return score / len(y_true) if normalize else int(score)\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "def save_checkpoint(model_state: dict,\n",
    "                    optim_state: dict,\n",
    "                    file_name: Union[str, Path],\n",
    "                    **params) -> None:\n",
    "    \"\"\"Checkpoint model params during training.\"\"\"\n",
    "    checkpoint = {\"model_state_dict\": model_state,\n",
    "                  \"optim_state_dict\": optim_state}\n",
    "    for key, val in params.items():\n",
    "        checkpoint[key] = val\n",
    "    torch.save(checkpoint, file_name)\n",
    "\n",
    "\n",
    "def load_checkpoint(file_name: Union[str, Path]) -> dict:\n",
    "    \"\"\"Retrieve saved model state dict.\"\"\"\n",
    "    return torch.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eventually, results need to be a list of 2028 0 or 1's\n",
    "#results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fasttext(model, data):\n",
    "    predictions = []\n",
    "    for text in data:\n",
    "        pred = model.predict(text)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_seqs_to_file(text, labels=None, file_name=\"test.txt\"):\n",
    "    with open(file_name, \"w\") as f:\n",
    "        if labels:\n",
    "            for seq, label in zip(text, labels):\n",
    "                f.write(f\"__label__{str(label)} {seq}\\n\")\n",
    "        else:\n",
    "            for seq in text:\n",
    "                f.write(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_seqs_to_file(traindata.X, traindata.y, \"data.train\")\n",
    "save_seqs_to_file(devdata.X, devdata.y, \"data.dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make final training data\n",
    "X_final = traindata.X.copy()\n",
    "X_final.extend(devdata.X.copy())\n",
    "y_final = traindata.y.copy()\n",
    "y_final.extend(devdata.y.copy())\n",
    "\n",
    "save_seqs_to_file(X_final, y_final, \"final.train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fasttext\n",
    "\n",
    "model = fasttext.train_supervised(input='data.train', autotuneValidationFile='data.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predictions = predict_fasttext(model, devdata.X)\n",
    "dev_score = accuracy_score_scalers(dev_predictions, devdata.y)\n",
    "dev_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.lrUpdateRate)\n",
    "print(model.lr)\n",
    "print(model.minCount)\n",
    "print(model.epoch)\n",
    "print(model.ws)\n",
    "print(model.wordNgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  20861\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1855690 lr:  0.000000 avg.loss:  0.132459 ETA:   0h 0m 0s100.0% words/sec/thread: 1856238 lr: -0.000013 avg.loss:  0.132459 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9052787370498273"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fasttext.train_supervised(\"data.train\", \n",
    "                                  lr=0.55, \n",
    "                                  lrUpdateRate=100, \n",
    "                                  minCount=1, \n",
    "                                  epoch=15, \n",
    "                                  ws=5,\n",
    "                                  wordNgrams=2)\n",
    "\n",
    "dev_predictions = predict_fasttext(model, devdata.X)\n",
    "dev_score = accuracy_score_scalers(dev_predictions, devdata.y)\n",
    "dev_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  21421\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2102197 lr:  0.000000 avg.loss:  0.121651 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(\"final.train\", \n",
    "                                  lr=0.55, \n",
    "                                  lrUpdateRate=100, \n",
    "                                  minCount=1, \n",
    "                                  epoch=15, \n",
    "                                  ws=5,\n",
    "                                  wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict_fasttext(model, testdata.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = fasttext.train_supervised(input=\"data.train\", epoch=20, wordNgrams=3, minCount=5)\n",
    "#dev_predictions = predict_fasttext(model, devdata.X)\n",
    "#dev_score = accuracy_score_scalers(dev_predictions, devdata.y)\n",
    "#dev_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for min_count in range(1, 50, 2):\n",
    "#     model = FastText.train_supervised(\"train.txt\", epoch=min_count, wordNgrams=2, minCount=2)\n",
    "\n",
    "#     dev_predictions = predict_fasttext(model, devdata.X)\n",
    "#     dev_score = accuracy_score_scalers(dev_predictions, devdata.y)\n",
    "\n",
    "#     train_predictions = predict_fasttext(model, traindata.X)\n",
    "#     train_score = accuracy_score_scalers(train_predictions, traindata.y)\n",
    "    \n",
    "#     print(f\"min count={min_count}\")\n",
    "#     print(f\"\\ttrain score={train_score}\")\n",
    "#     print(f\"\\tdev score={dev_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Prediction Result File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to submit a prediction result file. It should have 2028 lines, every line should be either 0 or 1, which is your model's prediction on the respective test set instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Union\n",
    "from pathlib import Path\n",
    "\n",
    "def save_results(results: Sequence[int],\n",
    "                 file_name: Union[str, Path] = \"upload_predictions.txt\") -> None:\n",
    "    \"\"\"Write final predictions to submission file.\"\"\"\n",
    "    with open(file_name, mode='w', encoding=\"utf-8\") as f:\n",
    "        for x in results:\n",
    "            out = str(int(x))\n",
    "            f.write(out + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for x in test_predictions:\n",
    "    if x[0][0] == \"__label__1\":\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose you had your model's predictions on the 2028 test cases read from test_enc_unlabeled.tsv, and \n",
    "#those results are in the list called 'results'\n",
    "assert (len(results) == 2028)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the results are not float numbers, but intergers 0 and 1\n",
    "#results = [int(x) for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your prediction results to 'upload_predictions.txt' and upload that later\n",
    "#with open('upload_predictions.txt', 'w', encoding = 'utf-8') as fp:\n",
    "#    for x in results:\n",
    "#        fp.write(str(x) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
